Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the datasets provided, explain the need to standardize the attribute values.
R1:The attribute values must be standardized for all attributes to contribute to the solution in an equal amount. Since the algorithms measure distance between points, it is important for the attributes to be in the same scale


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: We calculated the average and the standard deviation of the training set. For both training and testing sets, we subtracted the previously calculated average, and we divided by standard deviation


Q3: Classification: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3:We calculated the amount of elements of that class that exist in the training set.


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: We created 8 kdes, and each one takes one of the features, and each single of them gives a score to the sample. 4 of them are zero classifiers and 4 of them are one classifiers. The highest score of the classifiers is the one who is the final result


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5:The bandwidth is how much the Kernel density estimator raises its estimate when it finds new points. Therefore, a higher bandwidth value means that the probability values will raise faster. However, too high of values will cause the model to raise the estimate to values that are too high.


Q6: Explain how you determined the best bandwidth parameter for your classifier. You may include a relevant piece of your code if this helps you explain.
R6: We used an interval from 0.02 to 0.6 with a step of 0.02. For each value of the interval, we created 5 folds, and calculated the mean squared error for the validation set of each one of them. We then calculated the average of the mean squared error of the different folds and then chose the bandwidth with the lowest value


Q7: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R7: We used cross validation with 5 folds, and we calculated which model had the lowest average of the validation error of each one of the folds.


Q8: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the one provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R8: Naive Bayes with 0.3 Bandwidth /Library Gaussian NB:  True Error Estimate : 071 / 0.092 , Confidence interval of the approximate normal test:  86.0 +- 17.54 / 118.0 +- 20.26, McNemar test values : True 0 : 815/787 , False 0 : 65/86 , True 1 : 346/342 , False 1: 28/32 , Mcnemar Values: 10.4 / 25.7 , from this we can conclude that the Naive Bayes classifier appears to be better than the Gaussian NB classifier. However, since the two confidence intervals intersect, we can not exclude the hypothesis where they have the same error rate.


Q9: Regression: Explain what experiments and plots gave you good
evidence to choose a given model degree. 
R9:The plots that gave the most understanding of the best model degree to opt for were the Mean Squared Error VS Model Degree plot and also the individual predicted VS actual for each degree tested.


Q10: In the case of your mean squared error plot explain why one of the error
curves is always decreasing, while the other is not.
R10:In the mean squared error plot, the training error curve is always decreasing as the model degree increases because the model becomes incredibly specific and biased towards the training dataset, making accurate predictions in data the model already knows. On the contrary, the test error curve begins to increase at a certain point as the model degree increases because the model is not broad enough to correctly classify a sample that is not in the training datatset: if the sample from the test set deviates even a slight amount from the patterns recognized in the training phase, it is classified incorrectly.


Q11: In the plots of the true versus predicted values, where would be
all the points when predicted by an ideal regressor? Justify.
R11: All the values would be on the curve wit the slope of y=x because then the true value would be equal to the predicted value


Q12: Explain your validation procedure and comment on the true error
of your chosen model for unseen data.
R12:We split the data into test and train with 80 percent to train, and then redid the same again for the training set with 80 percent for training and 20 percent for validation. The true error for unseen data is better estimated with the predictions of the test set.


